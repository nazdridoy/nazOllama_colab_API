{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "olWxCQcDD7AQ"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@markdown <h3>Check GPU Availability</h3>\n",
        "\n",
        "! nvidia-smi\n",
        "! nvcc -V\n",
        "! free -h\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "S8Z8IE0Kkapu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WFSBllyblCF",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown <h3>Install Ollama and Cloudflare Tunnel</h3>\n",
        "import os\n",
        "import urllib.request\n",
        "from IPython.display import clear_output\n",
        "import subprocess\n",
        "\n",
        "HOME = os.path.expanduser(\"~\")\n",
        "\n",
        "# define helper functions\n",
        "\n",
        "def textAn(TEXT, ty='d'):\n",
        "      from IPython.display import HTML\n",
        "\n",
        "      if ty == 'd':\n",
        "            return display(HTML('''<style>@import url(https://fonts.googleapis.com/css?family=Raleway:400,700,900,400italic,700italic,900italic);#wrapper {   font: 17px 'Raleway', sans-serif;animation: text-shadow 1.5s ease-in-out infinite;    margin-left: auto;    margin-right: auto;    }#container {    display: flex;    flex-direction: column;    float: left;     }@keyframes text-shadow { 0% 20% {          transform: translateY(-0.1em);        text-shadow:             0 0.1em 0 #0c2ffb,             0 0.1em 0 #2cfcfd,             0 -0.1em 0 #fb203b,             0 -0.1em 0 #fefc4b;    }    40% {          transform: translateY(0.1em);        text-shadow:             0 -0.1em 0 #0c2ffb,             0 -0.1em 0 #2cfcfd,             0 0.1em 0 #fb203b,             0 0.1em 0 #fefc4b;    }       60% {        transform: translateY(-0.1em);        text-shadow:             0 0.1em 0 #0c2ffb,             0 0.1em 0 #2cfcfd,             0 -0.1em 0 #fb203b,             0 -0.1em 0 #fefc4b;    }   }@media (prefers-reduced-motion: reduce) {    * {      animation: none !important;      transition: none !important;    }}</style><div id=\"wrapper\"><div id=\"container\">'''+TEXT+'''</div></div>'''))\n",
        "      elif ty == 'twg':\n",
        "            textcover = str(len(TEXT)*0.55)\n",
        "            return display(HTML('''<style>@import url(https://fonts.googleapis.com/css?family=Anonymous+Pro);.line-1{font-family: 'Anonymous Pro', monospace;    position: relative;   border-right: 1px solid;    font-size: 15px;   white-space: nowrap;    overflow: hidden;    }.anim-typewriter{  animation: typewriter 0.4s steps(44) 0.2s 1 normal both,             blinkTextCursor 600ms steps(44) infinite normal;}@keyframes typewriter{  from{width: 0;}  to{width: '''+textcover+'''em;}}@keyframes blinkTextCursor{  from{border-right:2px;}  to{border-right-color: transparent;}}</style><div class=\"line-1 anim-typewriter\">'''+TEXT+'''</div>'''))\n",
        "\n",
        "def loadingAn(name=\"cal\"):\n",
        "      from IPython.display import HTML\n",
        "\n",
        "      if name == \"cal\":\n",
        "          return display(HTML('<style>.lds-ring {   display: inline-block;   position: relative;   width: 34px;   height: 34px; } .lds-ring div {   box-sizing: border-box;   display: block;   position: absolute;   width: 34px;   height: 34px;   margin: 4px;   border: 5px solid #cef;   border-radius: 50%;   animation: lds-ring 1.2s cubic-bezier(0.5, 0, 0.5, 1) infinite;   border-color: #cef transparent transparent transparent; } .lds-ring div:nth-child(1) {   animation-delay: -0.45s; } .lds-ring div:nth-child(2) {   animation-delay: -0.3s; } .lds-ring div:nth-child(3) {   animation-delay: -0.15s; } @keyframes lds-ring {   0% {     transform: rotate(0deg);   }   100% {     transform: rotate(360deg);   } }</style><div class=\"lds-ring\"><div></div><div></div><div></div><div></div></div>'))\n",
        "      elif name == \"lds\":\n",
        "          return display(HTML('''<style>.lds-hourglass {  display: inline-block;  position: relative;  width: 34px;  height: 34px;}.lds-hourglass:after {  content: \" \";  display: block;  border-radius: 50%;  width: 34px;  height: 34px;  margin: 0px;  box-sizing: border-box;  border: 20px solid #dfc;  border-color: #dfc transparent #dfc transparent;  animation: lds-hourglass 1.2s infinite;}@keyframes lds-hourglass {  0% {    transform: rotate(0);    animation-timing-function: cubic-bezier(0.55, 0.055, 0.675, 0.19);  }  50% {    transform: rotate(900deg);    animation-timing-function: cubic-bezier(0.215, 0.61, 0.355, 1);  }  100% {    transform: rotate(1800deg);  }}</style><div class=\"lds-hourglass\"></div>'''))\n",
        "\n",
        "loadingAn()\n",
        "textAn(\"Installing Ollama and Cloudflare Tunnel...\", \"twg\")\n",
        "\n",
        "# Install Ollama\n",
        "!curl https://ollama.ai/install.sh | sh\n",
        "\n",
        "# Install Cloudflared\n",
        "!curl -L --output cloudflared.deb https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared.deb\n",
        "!rm cloudflared.deb\n",
        "\n",
        "clear_output()\n",
        "print(\"‚úÖ Installation completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <h3>Start Ollama Server</h3>\n",
        "import os\n",
        "import signal\n",
        "import subprocess\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def start_ollama_server():\n",
        "    # Kill any existing ollama process\n",
        "    try:\n",
        "        subprocess.run(['pkill', 'ollama'], check=False)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Start ollama serve in background with GPU support\n",
        "    process = subprocess.Popen(\n",
        "        ['ollama', 'serve'],\n",
        "        env={\n",
        "            **os.environ,\n",
        "            'OLLAMA_HOST': '0.0.0.0:11434',\n",
        "            'CUDA_VISIBLE_DEVICES': '0',  # Use first GPU\n",
        "            'OLLAMA_CUDA': '1'  # Enable CUDA support\n",
        "        },\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE\n",
        "    )\n",
        "\n",
        "    # Wait briefly to ensure server starts\n",
        "    import time\n",
        "    time.sleep(2)\n",
        "\n",
        "    # Check if process is running\n",
        "    if process.poll() is None:\n",
        "        clear_output()\n",
        "        print(\"‚úÖ Ollama server is running with GPU support!\")\n",
        "        return process\n",
        "    else:\n",
        "        clear_output()\n",
        "        print(\"‚ùå Failed to start Ollama server\")\n",
        "        return None\n",
        "\n",
        "ollama_process = start_ollama_server()"
      ],
      "metadata": {
        "id": "1U02fprZjCoU",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <h3>Pull Ollama Model</h3>\n",
        "#@markdown Select a model to pull if not already present\n",
        "\n",
        "MODEL_NAME = \"llama3.2:3b\" #@param [\"llama3.2:3b\", \"deepseek-r1:14b\",\"deepseek-r1:8b\", \"qwen2.5:7b\", \"qwen2.5:14b\", \"mistral\", \"qwen2.5-coder:7b\", \"phi4:14b\", \"gemma:7b\", \"gemma2:9b\"] {allow-input: true}\n",
        "FORCE_PULL = False #@param {type:\"boolean\"}\n",
        "\n",
        "import subprocess\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def check_model_exists(model_name):\n",
        "    max_retries = 3\n",
        "    for _ in range(max_retries):\n",
        "        result = subprocess.run(['ollama', 'list'],\n",
        "                              capture_output=True,\n",
        "                              text=True,\n",
        "                              timeout=10)\n",
        "        if result.returncode == 0:\n",
        "            return model_name in result.stdout\n",
        "        time.sleep(2)\n",
        "    return False\n",
        "\n",
        "def pull_model(model_name, force=False):\n",
        "    if not force and check_model_exists(model_name):\n",
        "        print(f\"‚úÖ Model {model_name} is already pulled\")\n",
        "        return True\n",
        "\n",
        "    print(f\"üì• Pulling model {model_name}...\")\n",
        "    process = subprocess.Popen(\n",
        "        ['ollama', 'pull', model_name],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT  # Capture stderr as well\n",
        "    )\n",
        "\n",
        "    # Show progress with error handling\n",
        "    error_output = []\n",
        "    while True:\n",
        "        output = process.stdout.readline()\n",
        "        if output == b'' and process.poll() is not None:\n",
        "            break\n",
        "        if output:\n",
        "            line = output.decode().strip()\n",
        "            print(line)\n",
        "            if \"error\" in line.lower():\n",
        "                error_output.append(line)\n",
        "\n",
        "    exit_code = process.poll()\n",
        "    if exit_code == 0:\n",
        "        clear_output()\n",
        "        print(f\"‚úÖ Successfully pulled {model_name}\")\n",
        "        return True\n",
        "    else:\n",
        "        clear_output()\n",
        "        print(f\"‚ùå Failed to pull {model_name}\")\n",
        "        if error_output:\n",
        "            print(\"Error details:\")\n",
        "            for err in error_output[-3:]:  # Show last 3 error lines\n",
        "                print(f\"| {err}\")\n",
        "        return False\n",
        "\n",
        "# Add retry logic for the pull command\n",
        "max_retries = 2\n",
        "for attempt in range(max_retries + 1):\n",
        "    if pull_model(MODEL_NAME, FORCE_PULL):\n",
        "        break\n",
        "    if attempt < max_retries:\n",
        "        print(f\"Retrying pull ({attempt + 1}/{max_retries})...\")\n",
        "        time.sleep(5)"
      ],
      "metadata": {
        "id": "bVgYBdRMjNzZ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <h3>List Pulled Models</h3>\n",
        "\n",
        "import subprocess\n",
        "from IPython.display import clear_output, Markdown\n",
        "\n",
        "def get_pulled_models():\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            ['ollama', 'list'],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True\n",
        "        )\n",
        "        models = []\n",
        "        # Parse output, skip header line\n",
        "        for line in result.stdout.split('\\n')[1:]:\n",
        "            if line.strip():\n",
        "                model_name = line.split()[0]\n",
        "                models.append(model_name)\n",
        "        return models\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(\"Error retrieving model list:\")\n",
        "        print(e.stderr)\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "pulled_models = get_pulled_models()\n",
        "\n",
        "clear_output()\n",
        "if pulled_models:\n",
        "    print(\"‚úÖ Pulled Models:\")\n",
        "    display(Markdown(\"\\n\".join([f\"- ü¶ô {model}\" for model in pulled_models])))\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è No models pulled yet. Use the model pull section above first.\")\n"
      ],
      "metadata": {
        "id": "ANVM2dAImbqC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <h3>Expose Ollama API via Cloudflare Tunnel</h3>\n",
        "\n",
        "import socket\n",
        "import subprocess\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "import requests\n",
        "from IPython.display import clear_output, Markdown\n",
        "\n",
        "#@markdown ### API Settings\n",
        "API_KEY = \"\" #@param {type:\"string\"}\n",
        "WORKER_URL = \"https://nazkvhub.nazdridoy.workers.dev\" #@param {type:\"string\"}\n",
        "UNIQUE_KEY = \"ollama-tunnel\" #@param {type:\"string\"}\n",
        "\n",
        "def expose_ollama_api():\n",
        "    # Verify Ollama is running\n",
        "    if not ollama_process or ollama_process.poll() is not None:\n",
        "        print(\"‚ùå Ollama server not running! Start the server in Block 2 first.\")\n",
        "        return None\n",
        "\n",
        "    # Check API connectivity\n",
        "    try:\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "            s.settimeout(2)\n",
        "            s.connect((\"localhost\", 11434))\n",
        "    except:\n",
        "        print(\"‚ùå Ollama API not responding on port 11434\")\n",
        "        return None\n",
        "\n",
        "    # Start Cloudflare tunnel\n",
        "    process = subprocess.Popen(\n",
        "        [\"cloudflared\", \"tunnel\", \"--url\", \"http://localhost:11434\"],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        universal_newlines=True\n",
        "    )\n",
        "\n",
        "    # Wait for tunnel URL\n",
        "    public_url = None\n",
        "    start_time = time.time()\n",
        "    while time.time() - start_time < 30:  # 30 second timeout\n",
        "        line = process.stdout.readline()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        # Look for tunnel URL in output\n",
        "        match = re.search(r'https://[a-zA-Z0-9\\-]+\\.trycloudflare\\.com', line)\n",
        "        if match:\n",
        "            public_url = match.group(0)\n",
        "            break\n",
        "\n",
        "    if public_url:\n",
        "        clear_output()\n",
        "        display(Markdown(f\"### üöÄ Public Ollama API URL: [{public_url}]({public_url})\"))\n",
        "        print(\"Test with:\")\n",
        "        print(f'curl {public_url}/api/tags')\n",
        "        print(f'curl {public_url}/api/generate -d \\'{{\"model\":\"llama2\",\"prompt\":\"Hi\"}}\\'')\n",
        "\n",
        "        # Store URL in nazkvhub\n",
        "        try:\n",
        "            url_data = {\n",
        "                \"url\": public_url,\n",
        "                \"immutable\": False  # Allow updating when tunnel URL changes\n",
        "            }\n",
        "            response = requests.post(\n",
        "                f'{WORKER_URL}/v1/save/{UNIQUE_KEY}',\n",
        "                headers={\n",
        "                    'Authorization': f'Bearer {API_KEY}',\n",
        "                    'Content-Type': 'application/json'\n",
        "                },\n",
        "                json=url_data\n",
        "            )\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                print(f\"\\n‚úÖ URL stored in KV successfully\")\n",
        "                print(f\"Access at: {WORKER_URL}/v1/query/{UNIQUE_KEY}\")\n",
        "            else:\n",
        "                print(f\"\\n‚ùå Failed to store URL: {response.status_code}\")\n",
        "                print(f\"Response: {response.text}\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Error storing URL: {str(e)}\")\n",
        "\n",
        "        return public_url\n",
        "    else:\n",
        "        print(\"‚ùå Failed to create tunnel. Cloudflared output:\")\n",
        "        print(process.communicate()[0])\n",
        "        process.kill()\n",
        "        return None\n",
        "\n",
        "# Expose the API\n",
        "api_url = expose_ollama_api()"
      ],
      "metadata": {
        "id": "hbHoqsjy63_L",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <img src='http://icons.iconarchive.com/icons/blackvariant/button-ui-system-apps/1024/Terminal-icon.png' height=\"25\" alt=\"dedug\" />___Debug___"
      ],
      "metadata": {
        "id": "olWxCQcDD7AQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -X POST http://localhost:11434/api/generate -d '{\"model\":\"gemma2:2b\",\"prompt\":\"Hi\"}'"
      ],
      "metadata": {
        "id": "2HujXMHetSG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama rm gemma2:2b"
      ],
      "metadata": {
        "id": "JQseJ2XCYizW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps aux | grep ollama"
      ],
      "metadata": {
        "id": "X_3dyELx4o6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill ollama"
      ],
      "metadata": {
        "id": "b7O5wfjD3ROf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill cloudflared"
      ],
      "metadata": {
        "id": "y2zpqItFLdQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo ss -tunlp | grep \"LISTEN\""
      ],
      "metadata": {
        "id": "54oSZhBr3i__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install -g localtunnel"
      ],
      "metadata": {
        "id": "jq7xSlGA80lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lt --port 11434 &"
      ],
      "metadata": {
        "id": "7DCmQyRn9GOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://loca.lt/mytunnelpassword"
      ],
      "metadata": {
        "id": "vAII3r4V9sbV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}